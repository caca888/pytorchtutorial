{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string,re\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS=10000\n",
    "MAX_LEN=200\n",
    "BATCH_SIZE=20\n",
    "train_data_path = './data/imdb/train.tsv'\n",
    "test_data_path = './data/imdb/test.tsv'\n",
    "train_token_path = './data/imdb/train_token.tsv'\n",
    "test_token_path =  './data/imdb/test_token.tsv'\n",
    "train_samples_path = './data/imdb/train_samples/'\n",
    "test_samples_path =  './data/imdb/test_samples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = {}\n",
    "\n",
    "#清洗文本\n",
    "def clean_text(text):\n",
    "    lowercase = text.lower().replace(\"\\n\",\" \")\n",
    "    stripped_html = re.sub('<br />', ' ',lowercase)\n",
    "    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation),'',stripped_html)\n",
    "    return cleaned_punctuation\n",
    "\n",
    "with open(train_data_path,\"r\",encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        label,text = line.split(\"\\t\")\n",
    "        cleaned_text = clean_text(text)\n",
    "        for word in cleaned_text.split(\" \"):\n",
    "            word_count_dict[word] = word_count_dict.get(word,0)+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = \"count\"))\n",
    "df_word_dict = df_word_dict.sort_values(by = \"count\",ascending =False)\n",
    "\n",
    "df_word_dict = df_word_dict[0:MAX_WORDS-2] #  \n",
    "df_word_dict[\"word_id\"] = range(2,MAX_WORDS) #编号0和1分别留给未知词<unkown>和填充<padding>\n",
    "\n",
    "word_id_dict = df_word_dict[\"word_id\"].to_dict()\n",
    "\n",
    "df_word_dict.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data_list,pad_length):\n",
    "    padded_list = data_list.copy()\n",
    "    if len(data_list)> pad_length:\n",
    "         padded_list = data_list[-pad_length:]\n",
    "    if len(data_list)< pad_length:\n",
    "         padded_list = [1]*(pad_length-len(data_list))+data_list\n",
    "    return padded_list\n",
    "\n",
    "def text_to_token(text_file,token_file):\n",
    "    with open(text_file,\"r\",encoding = 'utf-8') as fin,\\\n",
    "      open(token_file,\"w\",encoding = 'utf-8') as fout:\n",
    "        for line in fin:\n",
    "            label,text = line.split(\"\\t\")\n",
    "            cleaned_text = clean_text(text)\n",
    "            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(\" \")]\n",
    "            pad_list = pad(word_token_list,MAX_LEN)\n",
    "            out_line = label+\"\\t\"+\" \".join([str(x) for x in pad_list])\n",
    "            fout.write(out_line+\"\\n\")\n",
    "        \n",
    "text_to_token(train_data_path,train_token_path)\n",
    "text_to_token(test_data_path,test_token_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(train_samples_path):\n",
    "    os.mkdir(train_samples_path)\n",
    "    \n",
    "if not os.path.exists(test_samples_path):\n",
    "    os.mkdir(test_samples_path)\n",
    "    \n",
    "    \n",
    "def split_samples(token_path,samples_dir):\n",
    "    with open(token_path,\"r\",encoding = 'utf-8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            with open(samples_dir+\"%d.txt\"%i,\"w\",encoding = \"utf-8\") as fout:\n",
    "                fout.write(line)\n",
    "            i = i+1\n",
    "\n",
    "split_samples(train_token_path,train_samples_path)\n",
    "split_samples(test_token_path,test_samples_path)\n",
    "print(os.listdir(train_samples_path)[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "class imdbDataset(Dataset):\n",
    "    def __init__(self,samples_dir):\n",
    "        self.samples_dir = samples_dir\n",
    "        self.samples_paths = os.listdir(samples_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples_paths)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        path = self.samples_dir + self.samples_paths[index]\n",
    "        with open(path,\"r\",encoding = \"utf-8\") as f:\n",
    "            line = f.readline()\n",
    "            label,tokens = line.split(\"\\t\")\n",
    "            label = torch.tensor([float(label)],dtype = torch.float)\n",
    "            feature = torch.tensor([int(x) for x in tokens.split(\" \")],dtype = torch.long)\n",
    "            return  (feature,label)\n",
    "ds_train = imdbDataset(train_samples_path)\n",
    "ds_test = imdbDataset(test_samples_path)\n",
    "\n",
    "print(len(ds_train))\n",
    "print(len(ds_test))\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = True,num_workers=0)\n",
    "dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features,labels in dl_train:\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torchkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.seed()\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class Net(torchkeras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量\n",
    "        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = 3,padding_idx = 1)\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))\n",
    "        self.conv.add_module(\"pool_1\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_1\",nn.ReLU())\n",
    "        self.conv.add_module(\"conv_2\",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))\n",
    "        self.conv.add_module(\"pool_2\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_2\",nn.ReLU())\n",
    "        \n",
    "        self.dense = nn.Sequential()\n",
    "        self.dense.add_module(\"flatten\",nn.Flatten())\n",
    "        self.dense.add_module(\"linear\",nn.Linear(6144,1))\n",
    "        self.dense.add_module(\"sigmoid\",nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "        \n",
    "\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "model.summary(input_shape = (200,),input_dtype = torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y_true):\n",
    "    y_pred = torch.where(y_pred>0.5,torch.ones_like(y_pred,dtype = torch.float32),\n",
    "                      torch.zeros_like(y_pred,dtype = torch.float32))\n",
    "    acc = torch.mean(1-torch.abs(y_true-y_pred))\n",
    "    return acc\n",
    "\n",
    "model.compile(loss_func = nn.BCELoss(),optimizer= torch.optim.Adagrad(model.parameters(),lr = 0.02),\n",
    "             metrics_dict={\"accuracy\":accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhistory = model.fit(2,dl_train,dl_val=dl_test,log_step_freq= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "plot_metric(dfhistory,\"loss\")\n",
    "plot_metric(dfhistory,\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=lambda x:re.sub('[%s]'%string.punctuation,\"\",x).split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLowFreqWords(arr,vocab):\n",
    "    arr=[[x if x<MAX_WORDS else 0 for x in example]\n",
    "         for example in arr ]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=torchtext.data.Field(sequential=True,tokenize=tokenizer,lower=True,fix_length=MAX_LEN,postprocessing=filterLowFreqWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL=torchtext.data.Field(sequential=False,use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train,ds_test=torchtext.data.TabularDataset.splits(\n",
    "    path='./data/imdb', train='train.tsv',test='test.tsv',format='tsv',\n",
    "    fields=[('label', LABEL), ('text',TEXT)],skip_header=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter=torchtext.data.Iterator.splits(\n",
    "    ds_train, sort_within_batch=True, sort_key=lambda x:len(x.text), \n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train[0].text)\n",
    "print(ds_train[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(TEXT.vocab))\n",
    "print(TEXT.vocab.itos[0])\n",
    "print(TEXT.vocab.itos[1])\n",
    "print(TEXT.vocab.stoi['<unk>'])\n",
    "print(TEXT.vocab.stoi['<pad>'])\n",
    "\n",
    "print(TEXT.vocab.freqs['<unk>'])\n",
    "print(TEXT.vocab.freqs['a'])\n",
    "print(TEXT.vocab.freqs['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_iter:\n",
    "    features=b.text\n",
    "    labels=b.label\n",
    "    print(features)\n",
    "    print(features.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
